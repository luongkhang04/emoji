\section{CONCLUSION}
This report presented an end-to-end emoji prediction system that spans data construction, model benchmarking, and practical deployment for interactive use. Starting from tweet-style text, we built a cleaned dataset of $C=43$ emoji classes and established a consistent evaluation protocol using top-$k$ accuracy to reflect the fact that multiple emojis may be plausible for the same message. :contentReference[oaicite:0]{index=0}

Across a diverse set of baselines---TF--IDF with linear classifiers, FastText-style subword models, TextCNN, and bidirectional RNNs with attention---we observed that strong sparse-feature methods remain competitive on short, noisy social text. However, the fine-tuned DistilBERT Transformer achieved the best ranking quality on the held-out test set, reaching $\mathrm{Acc@1}=0.2632$ and $\mathrm{Acc@5}=0.5659$, which makes it particularly suitable for suggestion-style interfaces where users benefit from multiple candidates. :contentReference[oaicite:1]{index=1}

Beyond model accuracy, we demonstrated a deployment pathway designed for real-world usability. The best checkpoint is exported to ONNX, optionally quantized, and executed client-side via Transformers.js and ONNX Runtime (WASM), enabling fast, privacy-preserving inference directly in the browser. We additionally provided both a static web demo and a lightweight browser extension to illustrate how the model can support emoji suggestions during typing without sending user text to a server. :contentReference[oaicite:2]{index=2}

\subsection{Limitations and Future Work}
Despite encouraging results, several limitations remain. First, the current task is formulated as \emph{single-label} classification, while real messages often contain multiple emojis (or none), and emoji usage can be highly user- and context-dependent. Second, the dataset is derived from scraped tweet-like text and may contain residual noise, topical bias, and platform-specific conventions that limit generalization to other domains (e.g., chat, forums, or multilingual settings). Third, our evaluation focuses on top-$k$ accuracy; for deployment, \emph{confidence calibration} and thresholding are also important for deciding when to suggest an emoji and how to rank candidates under uncertainty. :contentReference[oaicite:3]{index=3}

Future work could therefore explore: (i) multi-label and ``no-emoji'' modeling, (ii) improved calibration (e.g., temperature scaling) for more reliable probabilities and better UX, (iii) domain adaptation and multilingual training to broaden robustness, and (iv) personalization mechanisms that learn from user feedback while preserving privacy (e.g., on-device lightweight adapters). Finally, larger or more specialized pretrained encoders may further improve accuracy, but should be evaluated jointly with latency and model size constraints to maintain a responsive in-browser experience. :contentReference[oaicite:4]{index=4}